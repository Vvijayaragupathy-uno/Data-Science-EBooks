{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vvijayaragupathy-uno/Data-Science-EBooks/blob/main/Ollama_MMLU_Pro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "88WyLLrDBe-o",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19ec7706-e14f-41fb-855c-637e173c783a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Mar 13 13:00:40 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "#@title #Runtime Info\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "emySi6TEC-M8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29dff50d-5112-4d75-a08c-fe887c57fbca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  pci.ids usb.ids\n",
            "The following NEW packages will be installed:\n",
            "  lshw pci.ids usb.ids\n",
            "0 upgraded, 3 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 790 kB of archives.\n",
            "After this operation, 2,988 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 lshw amd64 02.19.git.2021.06.19.996aaad9c7-2build1 [321 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 pci.ids all 0.0~2022.01.22-1 [251 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 usb.ids all 2022.04.02-1 [219 kB]\n",
            "Fetched 790 kB in 1s (843 kB/s)\n",
            "Selecting previously unselected package lshw.\n",
            "(Reading database ... 124947 files and directories currently installed.)\n",
            "Preparing to unpack .../lshw_02.19.git.2021.06.19.996aaad9c7-2build1_amd64.deb ...\n",
            "Unpacking lshw (02.19.git.2021.06.19.996aaad9c7-2build1) ...\n",
            "Selecting previously unselected package pci.ids.\n",
            "Preparing to unpack .../pci.ids_0.0~2022.01.22-1_all.deb ...\n",
            "Unpacking pci.ids (0.0~2022.01.22-1) ...\n",
            "Selecting previously unselected package usb.ids.\n",
            "Preparing to unpack .../usb.ids_2022.04.02-1_all.deb ...\n",
            "Unpacking usb.ids (2022.04.02-1) ...\n",
            "Setting up pci.ids (0.0~2022.01.22-1) ...\n",
            "Setting up lshw (02.19.git.2021.06.19.996aaad9c7-2build1) ...\n",
            "Setting up usb.ids (2022.04.02-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Cloning into 'Ollama-MMLU-Pro'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 10 (delta 0), reused 4 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (10/10), 17.22 KiB | 17.22 MiB/s, done.\n",
            "/content/Ollama-MMLU-Pro\n",
            "Collecting datasets (from -r requirements.txt (line 1))\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (1.61.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (0.10.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 1)) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 1)) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->-r requirements.txt (line 1))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 1)) (4.67.1)\n",
            "Collecting xxhash (from datasets->-r requirements.txt (line 1))\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->-r requirements.txt (line 1))\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 1)) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 1)) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r requirements.txt (line 2)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r requirements.txt (line 2)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r requirements.txt (line 2)) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai->-r requirements.txt (line 2)) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai->-r requirements.txt (line 2)) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.18.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 2)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 2)) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->-r requirements.txt (line 1)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->-r requirements.txt (line 1)) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 1)) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "#@title # Setup\n",
        "%cd /content/\n",
        "!apt install lshw\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!git clone --depth 1 https://github.com/chigkim/Ollama-MMLU-Pro\n",
        "%cd Ollama-MMLU-Pro\n",
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Run server\n",
        "\n",
        "%env OLLAMA_NUM_PARALLEL=16\n",
        "%env OLLAMA_FLASH_ATTENTION=1\n",
        "\n",
        "import threading\n",
        "import subprocess\n",
        "server = threading.Thread(target=lambda: subprocess.run(['ollama', 'serve'], check=True), daemon=True)\n",
        "server.start()\n",
        "import  time\n",
        "\n",
        "time.sleep(2)"
      ],
      "metadata": {
        "id": "q9IV1xQao-We",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "402f381a-700e-4107-e7ae-b9c5dc89f8be"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: OLLAMA_NUM_PARALLEL=16\n",
            "env: OLLAMA_FLASH_ATTENTION=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download\n",
        "print(\"Downloading model...\")\n",
        "!ollama pull llama3:8b-instruct-q8_0 &>/dev/null\n"
      ],
      "metadata": {
        "id": "-8jD_-APlf51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbc05dc1-30c1-4255-ebb7-58a7d51f2ea0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Test\n",
        "\n",
        "!rm -rf eval_results\n",
        "!python run_openai.py \\\n",
        "    --url http://localhost:11434/v1 \\\n",
        "    --model llama3:8b-instruct-q8_0 \\\n",
        "    --category 'computer science' \\\n",
        "    --verbosity 0 \\\n",
        "    --parallel 16\n"
      ],
      "metadata": {
        "id": "iNvYwyAJcNnz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6069fc4f-ab9b-4216-8e6c-3c950bc6e13e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-13 12:22:05.366657\n",
            "{\n",
            "\t\"comment\": \"\",\n",
            "\t\"server\": {\n",
            "\t\t\"url\": \"http://localhost:11434/v1\",\n",
            "\t\t\"model\": \"llama3:8b-instruct-q8_0\",\n",
            "\t\t\"timeout\": 600.0\n",
            "\t},\n",
            "\t\"inference\": {\n",
            "\t\t\"temperature\": 0.0,\n",
            "\t\t\"top_p\": 1.0,\n",
            "\t\t\"max_tokens\": 2048,\n",
            "\t\t\"system_prompt\": \"The following are multiple choice questions (with answers) about {subject}. Think step by step and then finish your answer with \\\"the answer is (X)\\\" where X is the correct letter choice.\",\n",
            "\t\t\"style\": \"multi_chat\"\n",
            "\t},\n",
            "\t\"test\": {\n",
            "\t\t\"subset\": 1.0,\n",
            "\t\t\"parallel\": 16\n",
            "\t},\n",
            "\t\"log\": {\n",
            "\t\t\"verbosity\": 0,\n",
            "\t\t\"log_prompt\": true\n",
            "\t}\n",
            "}\n",
            "README.md: 100% 10.9k/10.9k [00:00<00:00, 49.7MB/s]\n",
            "test-00000-of-00001.parquet: 100% 4.15M/4.15M [00:00<00:00, 40.3MB/s]\n",
            "validation-00000-of-00001.parquet: 100% 45.3k/45.3k [00:00<00:00, 115MB/s]\n",
            "Generating test split: 100% 12032/12032 [00:00<00:00, 109982.40 examples/s]\n",
            "Generating validation split: 100% 70/70 [00:00<00:00, 18279.25 examples/s]\n",
            "assigned subjects ['computer science']\n",
            "Testing computer science...\n",
            "100% 410/410 [13:41<00:00,  2.00s/it]\n",
            "Finished testing computer science in 13 minutes 42 seconds.\n",
            "Total, 163/410, 39.76%\n",
            "Random Guess Attempts, 11/410, 2.68%\n",
            "Correct Random Guesses, 2/11, 18.18%\n",
            "Adjusted Score Without Random Guesses, 161/399, 40.35%\n",
            "Finished the benchmark in 13 minutes 45 seconds.\n",
            "Total, 163/410, 39.76%\n",
            "Random Guess Attempts, 11/410, 2.68%\n",
            "Correct Random Guesses, 2/11, 18.18%\n",
            "Adjusted Score Without Random Guesses, 161/399, 40.35%\n",
            "Token Usage:\n",
            "Prompt tokens: min 1418, average 1556, max 2042, total 637820, tk/s 772.64\n",
            "Completion tokens: min 30, average 161, max 2048, total 66091, tk/s 80.06\n",
            "Markdown Table:\n",
            "| overall | computer science |\n",
            "| ------- | ---------------- |\n",
            "| 39.76 | 39.76 |\n",
            "Report saved to: eval_results/llama3-8b-instruct-q8_0/report.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"eval_results/llama3-8b-instruct-q8_0/report.txt\"\n",
        "with open(file_path, \"r\") as file:\n",
        "    content = file.read()\n",
        "print(content)"
      ],
      "metadata": {
        "id": "O4XMImFNhEd3",
        "outputId": "65eb7bcb-bcb7-4c71-cfc5-c82dafc68997",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(file_path)"
      ],
      "metadata": {
        "id": "LszjQ_OShjnN",
        "outputId": "9f2e7767-5928-484f-bec2-efa9164067bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_399a0572-a789-4c1f-a9b8-36afb93e98c6\", \"report.txt\", 0)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "| overall | computer science |\n",
        "| ------- | ---------------- |\n",
        "| 39.76   | 39.76            |"
      ],
      "metadata": {
        "id": "6ndbu0aMlwQW",
        "outputId": "8fde2252-68f5-4a0b-a016-07a593226a08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-13-c8bb3e7cfc73>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-c8bb3e7cfc73>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    | overall | computer science |\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(subjects):\n",
        "    test_df, dev_df = load_mmlu_pro()\n",
        "    if not subjects:\n",
        "        subjects = list(test_df.keys())\n",
        "    print(\"Assigned subjects:\", subjects)\n",
        "    lock = threading.Lock()\n",
        "    system_prompt = config[\"inference\"][\"system_prompt\"]\n",
        "    wrong_answers = []  # List to store wrong answers\n",
        "\n",
        "    for subject in subjects:\n",
        "        start = time.time()\n",
        "        print(f\"Testing {subject}...\")\n",
        "        config[\"inference\"][\"system_prompt\"] = system_prompt.replace(\"{subject}\", subject)\n",
        "        test_data = test_df[subject]\n",
        "        output_res_path = os.path.join(output_dir, subject + \"_result.json\")\n",
        "        output_summary_path = os.path.join(output_dir, subject + \"_summary.json\")\n",
        "        res, category_record = update_result(output_res_path, lock)\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=config[\"test\"][\"parallel\"]) as executor:\n",
        "            futures = {\n",
        "                executor.submit(run_single_question, each, dev_df, res): each\n",
        "                for each in test_data\n",
        "            }\n",
        "            for future in tqdm(\n",
        "                as_completed(futures), total=len(futures), smoothing=0.0, ascii=True\n",
        "            ):\n",
        "                each = futures[future]\n",
        "                label = each[\"answer\"]\n",
        "                category = subject\n",
        "                prompt, response, pred, exist = future.result()\n",
        "                if exist:\n",
        "                    continue\n",
        "                if response is not None:\n",
        "                    res, category_record = update_result(output_res_path, lock)\n",
        "                    if category not in category_record:\n",
        "                        category_record[category] = {\"corr\": 0.0, \"wrong\": 0.0}\n",
        "                    if config[\"log\"][\"log_prompt\"]:\n",
        "                        each[\"prompt\"] = prompt\n",
        "                    each[\"response\"] = response\n",
        "                    each[\"pred\"] = pred\n",
        "                    res.append(each)\n",
        "                    if pred is not None:\n",
        "                        if pred == label:\n",
        "                            category_record[category][\"corr\"] += 1\n",
        "                        else:\n",
        "                            category_record[category][\"wrong\"] += 1\n",
        "                            # Add wrong answer to the list\n",
        "                            wrong_answers.append({\n",
        "                                \"category\": category,\n",
        "                                \"question\": each[\"question\"],\n",
        "                                \"options\": each[\"options\"],\n",
        "                                \"pred\": pred,\n",
        "                                \"answer\": label,\n",
        "                            })\n",
        "                    else:\n",
        "                        category_record[category][\"wrong\"] += 1\n",
        "                    save_res(res, output_res_path, lock)\n",
        "                    save_summary(category_record, output_summary_path, lock)\n",
        "                    res, category_record = update_result(output_res_path, lock)\n",
        "        save_res(res, output_res_path, lock)\n",
        "        log(f\"Finished testing {subject} in {elapsed(start)}.\")\n",
        "        save_summary(category_record, output_summary_path, lock, report=True)\n",
        "\n",
        "    # Display wrong answers\n",
        "    if wrong_answers:\n",
        "        print(\"\\nWrong Answers:\")\n",
        "        for i, wrong_answer in enumerate(wrong_answers, 1):\n",
        "            print(f\"\\nWrong Answer {i}:\")\n",
        "            print(f\"Category: {wrong_answer['category']}\")\n",
        "            print(f\"Question: {wrong_answer['question']}\")\n",
        "            print(f\"Options: {wrong_answer['options']}\")\n",
        "            print(f\"Model Prediction: {wrong_answer['pred']}\")\n",
        "            print(f\"Correct Answer: {wrong_answer['answer']}\")\n",
        "    else:\n",
        "        print(\"\\nNo wrong answers found.\")"
      ],
      "metadata": {
        "id": "s6JI6HRtdRRt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save wrong answers to a file\n",
        "if wrong_answers:\n",
        "    wrong_answers_path = os.path.join(output_dir, \"wrong_answers.json\")\n",
        "    with open(wrong_answers_path, \"w\") as f:\n",
        "        json.dump(wrong_answers, f, indent=4)\n",
        "    print(f\"\\nWrong answers saved to: {wrong_answers_path}\")"
      ],
      "metadata": {
        "id": "zh7Z2IuJdTlX",
        "outputId": "33acbc6a-d1bf-4fb7-bcb4-14cac75015bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'wrong_answers' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-1b2ee4fb9344>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save wrong answers to a file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mwrong_answers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mwrong_answers_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wrong_answers.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrong_answers_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrong_answers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'wrong_answers' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}